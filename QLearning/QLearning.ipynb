{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406b17ed",
   "metadata": {},
   "source": [
    "## QLearing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a21a0",
   "metadata": {},
   "source": [
    "Q值：用来衡量在某一状态下采取某一动作的好坏的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68659c",
   "metadata": {},
   "source": [
    "### 算法核心：  \n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a} Q(s', a) - Q(s, a) \\right]\n",
    "$$\n",
    "在最初阶段Q值表中的值全部都是随便赋值的  \n",
    "\n",
    "目前我们处在状态s，随机初始化各个动作对应的Q值，我们得到了一个Q值表(1)； \n",
    "\n",
    "然后model会根据ε-greedy策略执行动作，执行结束后进入一个新的状态s'，并得到了奖励r;  \n",
    "\n",
    "其中我们得到的s对应的Q值表是Q(s, a)的***估计值***，然后开始回溯更新Q值，取下一个状态s'的最大Q值 ***（这个时候还没有做出s'的动作）***：$r + \\gamma \\max_{a} Q(s', a)$，这也就是Q(s, a)的***现实值***，然后现实值减去估计值乘以学习率(α)，表示这个误差中有多少是要被学习的。原Q值（估计值）加上学习值矫正之后得到的就是更新之后的Q值。  \n",
    "\n",
    "总体来说就是\n",
    "$$\n",
    "Q_{updated} = Q_{previous} + \\alpha \\cdot ERROR(误差)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a1131",
   "metadata": {},
   "source": [
    "### 核心参数\n",
    "**γ：**  \n",
    "$$\n",
    "Q(s_1) = r_2 + \\gamma Q(s_2) = r_2 + \\gamma [r_3 + \\gamma Q(s_3)] = r_2 + \\gamma [r_3 + \\gamma [r_4 + \\gamma Q(s_4)]] = \\ldots\n",
    "$$\n",
    "$$\n",
    "Q(s_1) = r_2 + \\gamma r_3 + \\gamma^2 r_4 + \\gamma^3 r_5 + \\gamma^4 r_6 + \\ldots\n",
    "$$\n",
    "由此可见Q(s1) 和之后所有的奖励都有关，但是越往后的奖励衰减得越厉害；  \n",
    "当γ=1时将眼前的利益和之后的利益一视同仁；当γ=0时不考虑之后的利益，完全考虑当前的利益；  \n",
    "所以γ就是模型对于未来的利益的掂量  \n",
    "\n",
    "**α：**  \n",
    "α越大，表示对新获得的经验的信任程度越高，Q值变化越快。  \n",
    "α越小，表示更依赖原有Q值，学习速度变慢，但能更平稳地收敛。  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
