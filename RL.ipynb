{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061994c6",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0fea54",
   "metadata": {},
   "source": [
    "**核心要素：**  \n",
    "环境（Environment）：智能体所处的世界。  \n",
    "状态（State, s）：当前环境对智能体可见的信息。  \n",
    "动作（Action, a）：智能体可执行的操作。  \n",
    "奖励（Reward, r）：环境对动作好坏的即时反馈，可能稀疏或延迟。  \n",
    "策略（Policy, π）：从状态到动作的映射，决定如何行动。  \n",
    "价值函数（Value/Q）：评估某状态或状态-动作在长期回报上的好坏。  \n",
    "\n",
    "**基本交互循环：**  \n",
    "观察状态 s  \n",
    "按策略 π 选择动作 a  \n",
    "环境转移到新状态 s' 并给出奖励 r  \n",
    "用 (s, a, r, s') 更新价值估计或直接更新策略  \n",
    "重复，目标是最大化长期累计奖励  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333b2f7",
   "metadata": {},
   "source": [
    "**分类**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
