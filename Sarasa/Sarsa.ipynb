{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25817e6",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11329",
   "metadata": {},
   "source": [
    "Sarsa和QLearing最大的区别在于更新Q的过程，QLearning使用的仅仅是下一个状态Q值最大的动作，而实际情况下在s'状态不一定会执行这个Q值最大的动作，只关心了理论上选哪个动作最优秀。  \n",
    "\n",
    "但是Sarsa取的就是接下来要做的动作的Q值，而不是最大值，其他和QLearning就都一样了  \n",
    "\n",
    "也就是QLearning是还没有做出下一步动作时开始更新目前这一步的Q值，***先更新后选动作***（并不关心你下一步实际选了哪个动作，而是假设你总是选最优的动作来更新Q值），Sarsa是在做了下一步动作之后再更新这一步的Q值，***先选动作后更新***。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161df0e2",
   "metadata": {},
   "source": [
    "## Sarsa(λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f4d4d",
   "metadata": {},
   "source": [
    "Sarsa和QLearing都是但单步更新，记作Sarsa(0),但是Sarsa(λ)指的是在一个回合内走n步，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca8835",
   "metadata": {},
   "source": [
    "从第0步开始的回合\n",
    "$$ \n",
    "G_t^{\\lambda} = (1-\\lambda)\\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)} \n",
    "$$ \n",
    "$$（这是一种加权平均，离起点越近权重越大）$$\n",
    "$G_t^{(n)}$指的是这n步的奖励之和：\n",
    "$$\n",
    "G_t^{(n)} = r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1} r_{t+n} + \\gamma^n Q(s_{t+n}, a_{t+n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eb6f3",
   "metadata": {},
   "source": [
    "$举例：（6步3回合）$  \n",
    "\n",
    "***第一回合：*** \n",
    "\n",
    "步：  \n",
    "步0: s0, a0, r1=1  \n",
    "步1: s1, a1, r2=2  \n",
    "步2: s2, a2, r3=3  \n",
    "\n",
    "3步总回报： $$ G_0^{(3)} = r_1 + \\gamma r_2 + \\gamma^2 r_3 = 1 + 2 + 3 = 6 $$\n",
    "λ回报：$$ G_0^{\\lambda} = (1-\\lambda)\\left[ G_0^{(1)} + \\lambda G_0^{(2)} + \\lambda^2 G_0^{(3)} \\right] = 0.5 \\left[ 1 + 0.5 \\times 3 + 0.25 \\times 6 \\right] = 0.5 \\times [1 + 1.5 + 1.5] = 0.5 \\times 4 = 2 $$\n",
    "\n",
    "***第二回合：***  \n",
    "步：  \n",
    "步3: s3, a3, r4=4  \n",
    "步4: s4, a4, r5=5  \n",
    "步5: s5, a5, r6=6  \n",
    "\n",
    "3步总回报： $$ G_3^{(3)} = r_4 + \\gamma r_5 + \\gamma^2 r_6 = 4 + 5 + 6 = 15 $$\n",
    "λ回报：$$ G_3^{\\lambda} = 0.5 \\left[ 4 + 0.5 \\times 9 + 0.25 \\times 15 \\right] = 0.5 \\times [4 + 4.5 + 3.75] = 0.5 \\times 12.25 = 6.125 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad51f18",
   "metadata": {},
   "source": [
    "更新过程：  \n",
    "再每回合结束后进行以下操作：\n",
    "$$Q(S_t,A_t) ← Q(S_t,A_t) + α [ G_t^\\lambda − Q(S_t,A_t) ]$$\n",
    "\n",
    "每一步都有一个$G_t^\\lambda$,用于更新$Q(S_t,A_t)$,公式上面都写清楚了"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
